# -*- coding: utf-8 -*-
"""multiclass_DDI (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JWLx3v7fLStvWedzYRsb6FmLDuxx0Vs0

# Multiclass Classification for Drug Interaction
"""











import re
import numpy as np
import pandas as pd
import seaborn as sns
import itertools
from rdkit import Chem
from rdkit.Chem import AllChem
from thefuzz import process
import matplotlib.pyplot as plt

from tensorflow import keras
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import load_model

from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

df=pd.read_csv("/content/updated_file1.csv")

df.drop('index',axis= 'columns',inplace= True)

df.head(2)

label_encoder = LabelEncoder()
df['id'] = label_encoder.fit_transform(df['id'])

for column in df.columns:
    if df[column].is_unique:
        print(f"{column}: All values are unique")
    else:
        print(f"{column}: Contains duplicates")

def get_unique(df, column):
    features = set()
    for item in df[column].str.split('|'):
        features.update(item)
    return sorted(features)

def to_vector(feature_str, unique_values, vector_size):
    values = feature_str.split('|')
    vector = np.zeros(vector_size)
    for i in values:
        if i in unique_values:
            index = unique_values.index(i)
            vector[index] = 1
    return vector

def preprocess_column(df, column_name):
    unique_values = get_unique(df, column_name)
    vector_size = len(unique_values)
    vectors = df[column_name].apply(
        lambda x: to_vector(x, unique_values, vector_size)
    )

    column_df = pd.DataFrame(
        vectors.tolist(),
        columns=[f'{column_name}_{i}' for i in range(vector_size)]
    )
    column_df=column_df.astype(int)

    result_df = pd.concat([df, column_df], axis=1)

    return result_df, unique_values, column_df

df,T_uniqe, column_vec_T = preprocess_column(df, 'target')
df,E_uniqe, column_vec_E = preprocess_column(df, 'enzyme')
df,P_uniqe, column_vec_P = preprocess_column(df, 'pathway')

df.drop('enzyme',axis= 'columns',inplace= True)
df.drop('target',axis= 'columns',inplace= True)
df.drop('pathway',axis= 'columns',inplace= True)

fingerprints = []

# Loop through the 'smile' column and process each SMILES string
for smiles in df['smile']:
    # Ensure that the SMILES is a valid string and not NaN or a float
    if isinstance(smiles, str) and smiles:  # Check if it's a valid string and not empty
        # Convert the SMILES string to a molecule object
        mol = Chem.MolFromSmiles(smiles)

        # Check if the molecule is valid
        if mol:
            # Generate Morgan fingerprints with a radius of 2 and size of 1024 bits
            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)

            # Convert the fingerprint to a numpy array
            fingerprint_array = np.array(fingerprint)

            # Append the fingerprint to the list
            fingerprints.append(fingerprint_array)
        else:
            print(f"Invalid SMILES: {smiles}")
            fingerprints.append(None)
    else:
        print(f"Skipping invalid or missing SMILES: {smiles}")
        fingerprints.append(None)

fingerprints_df = pd.DataFrame(fingerprints)
fingerprints_df.columns = [f"smile_{i}" for i in range(fingerprints_df.shape[1])]
# Merge the fingerprints with the original DataFrame (removing the 'smile' column)
df = pd.concat([df.drop(columns=['smile']), fingerprints_df], axis=1)
column_vec_s=fingerprints_df
# Save the updated DataFrame to the same variable
df.reset_index(drop=True, inplace=True)

print("target vector size:", df.shape[1] - len(df.columns) + len(T_uniqe))
print("enzyme vector size:", df.shape[1] - len(df.columns) + len(E_uniqe))
print("pathway vector size:", df.shape[1] - len(df.columns) + len(P_uniqe))
print("SMAIl Vector size:",fingerprints_df.shape[1])

def Jaccard(matrix):
        matrix = np.array(matrix)
        numerator = matrix @ matrix.T
        denominator = np.ones(np.shape(matrix)) @ matrix.T + matrix @ np.ones(
            np.shape(matrix.T)) - matrix @ matrix.T
        return numerator / denominator

Target_matrix= Jaccard(column_vec_T)
enzem_matrix= Jaccard(column_vec_E)
pathway_matrix= Jaccard(column_vec_P)
smail_matrix =Jaccard(column_vec_s)

Target = Target_matrix.flatten()
Enzyme = enzem_matrix.flatten()
Pathway = pathway_matrix.flatten()
SMAIl = smail_matrix.flatten()

data= df[['id', 'name']]
data = pd.DataFrame(data, columns=["id", "name"])
new_df = data.merge(data, how="cross")
new_df = new_df.rename(columns={"id_x": "id_1", "name_x": "name_1", "id_y": "id_2", "name_y": "name_2"})

new_df["Target"] = Target
new_df["Enzyme"] = Enzyme
new_df["Pathway"] = Pathway
new_df["SMAIl"] = SMAIl

new_df

"""# data_labels

"""

interaction= pd.read_excel("/content/df_interaction.xlsx")

dic=interaction.drop(columns=['Unnamed: 0','index','index','interaction'],axis= 'columns')

dic['id1'] = label_encoder.fit_transform(dic['id1'])
dic['id2']=label_encoder.fit_transform(dic['id2'])

interaction.drop(columns=['Unnamed: 0','index'],axis= 'columns',inplace= True)

label_encoder = LabelEncoder()
interaction['id1'] = label_encoder.fit_transform(interaction['id1'])
interaction['id2']=label_encoder.fit_transform(interaction['id2'])
interaction["name1"] = label_encoder.fit_transform(interaction["name1"])
interaction["name2"] = label_encoder.fit_transform(interaction["name2"])

interaction = interaction.rename(columns={"id1": "id_1", "name1": "name_1", "id2": "id_2", "name2": "name_2"})

file1_sorted = interaction.sort_values(by=['id_1', 'id_2'])
file2_sorted = new_df.sort_values(by=['id_1', 'id_2'])
merged = pd.merge(file2_sorted, file1_sorted[['id_1', 'id_2', 'interaction']], on=['id_1', 'id_2'], how='left')
df = merged.dropna(subset=['interaction'])

df.head(2)

df2 = pd.read_excel("/content/df_event.xlsx")

df2['label']=df2['Unnamed: 0']

df2.drop('Unnamed: 0',axis= 'columns',inplace= True)

df2.head(2)

label_encoder = LabelEncoder()
df["name_1"] = label_encoder.fit_transform(df["name_1"])
df["name_2"] = label_encoder.fit_transform(df["name_2"])

tfidf_vectorizer = TfidfVectorizer(max_features=50)
interaction_tfidf = tfidf_vectorizer.fit_transform(df["interaction"])
interaction_tfidf_df = pd.DataFrame(interaction_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

interaction_tfidf_df.head(2)

df = df.reset_index(drop=True)
interaction_tfidf_df = interaction_tfidf_df.reset_index(drop=True)
df = pd.concat([df, interaction_tfidf_df], axis=1)

df.head(2)

print("Final dataset shape:", df.shape)

event_patterns = df2["event"].astype(str).str.lower().str.strip().tolist()
drug_names_set = set(df["name_1"].dropna().astype(str).tolist() + df["name_2"].dropna().astype(str).tolist())

def replace_drug_names(text, drug_list):
    if not isinstance(text, str):
        return text
    pattern = r'\b(' + '|'.join(map(re.escape, drug_list)) + r')\b'
    return re.sub(pattern, 'name', text, flags=re.IGNORECASE)

df["normalized_interaction"] = df["interaction"].astype(str).str.lower().str.strip().apply(lambda x: replace_drug_names(x, drug_names_set))

def fuzzy_match(normalized_text, event_list, threshold=80):
    match, score = process.extractOne(normalized_text, event_list)
    return match if score >= threshold else "Unknown"

df["label"] = df["normalized_interaction"].apply(lambda x: fuzzy_match(x, event_patterns))
df["num_label"] = df["label"].astype("category").cat.codes

df.head(2)

len(df['label'].unique())

plt.figure(figsize=(12, 6))
sns.countplot(x=df['num_label'])
plt.title('Class Distribution')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

def combine_minority_classes(df, target_column, threshold):

    class_counts = df[target_column].value_counts()
    minority_classes = class_counts[class_counts < threshold].index
    df_combined = df.copy()
    df_combined.loc[df_combined[target_column].isin(minority_classes), target_column] = 0

    return df_combined

df= combine_minority_classes(df, 'num_label', 100)

plt.figure(figsize=(12, 6))
sns.countplot(x=df['num_label'])
plt.title('Class Distribution')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

len(df['num_label'].unique())

df['num_label'].value_counts()

df = df[df['num_label'] != 0]

df.head(2)

def relabel_data(df, target_column):

    unique_labels = sorted(df[target_column].unique())
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
    df[target_column] = df[target_column].map(label_mapping)

    return df

df= relabel_data(df, 'num_label')

print(df['num_label'].unique())

X = df.drop(['num_label','interaction','normalized_interaction','label'], axis=1)
y = df['num_label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X.head(2)

X.to_csv("X_data.csv", index=False)



drug_dict = {}
for index, row in dic.iterrows():
    drug_dict[row["name1"]] = row["id1"]
    drug_dict[row["name2"]] = row["id2"]

joblib.dump(drug_dict, "drug_dictionary.pkl")

# Normalize data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import joblib
joblib.dump(scaler, "scaler.pkl")


#def focal_loss(alpha=0.25, gamma=2.0):
 #   def loss_fn(y_true, y_pred):
        # Ensure y_true is 1-dimensional (remove extra dimension if present)
     #   y_true = tf.squeeze(y_true, axis=1)  # Remove the extra dimension if it's there.
     #   y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])
     #   cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
     #   probs = tf.reduce_sum(y_true * y_pred, axis=-1)
     #   focal_weight = alpha * (1 - probs) ** gamma
      #  return focal_weight * cross_entropy
   # return loss_fn

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    Dropout(0.3),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(128, activation='relu'),
    BatchNormalization(),

    Dense(len(y_train.unique()), activation='softmax')
])

# Compile your model with Focal Loss
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, lr_reducer])

import tensorflow as tf

print("TensorFlow Version:", tf.__version__)
print("Keras Version:", tf.keras.__version__)

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_classes))

model.save('DDi.h5')

model = load_model("DDi.h5", custom_objects={"loss_fn": focal_loss(alpha=0.25, gamma=2.0)})

model.save('DDi.keras')  # استخدم .keras كامتداد

def return_id(d1,d2,dic):
    for index, row in dic.iterrows():
        if row['name1'] == d1 and row['name2'] == d2:
            return row['id1'],row['id2']
    return -1, -1

def test_model(model, id1, id2, X, scaler):

    if (X['id_1'] == id1).any() and (X['id_2'] == id2).any():
        # Find the matching row index
        matching_index = X[(X['id_1'] == id1) & (X['id_2'] == id2)].index[0]

        # Select the row using .loc[]
        X_test = X.loc[[matching_index]]

        # Scale the data using the provided scaler
        X_test = scaler.transform(X_test)

        # Reshape X_test to have a sample dimension
        X_test = X_test.reshape(1, -1)

        y_pred = model.predict(X_test)
        y_pred_class = np.argmax(y_pred)
        return y_pred_class

    else:
        return  -1 
    
    

def model_predict(drug1, drug2):
    id1, id2 = return_id(drug1, drug2, dic)
    if (X['id_1'] == id1).any() and (X['id_2'] == id2).any():
        idx = X[(X['id_1'] == id1) & (X['id_2'] == id2)].index[0]
        X_input = X.loc[[idx]]
        X_scaled = scaler.transform(X_input)
        pred = model.predict(X_scaled)
        predicted_class = np.argmax(pred)
        return int(predicted_class)
    else:
        return "Pair not found"
